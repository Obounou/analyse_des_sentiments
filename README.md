# ğŸ“Š Analyse des Sentiments

## ğŸ“ Description
Ce projet vise Ã  analyser les sentiments des avis clients Ã  l'aide du **NLP** et de **BERT**. Il met en place un pipeline comprenant plusieurs Ã©tapes : **extraction des donnÃ©es**, **prÃ©traitement**, **tokenisation** et **infÃ©rence** pour obtenir des insights exploitables.

## ğŸš€ Pipeline du projet
1. **Extraction des donnÃ©es** : Chargement et validation des avis clients depuis un fichier CSV.
2. **PrÃ©traitement des donnÃ©es** : Nettoyage du texte (suppression des caractÃ¨res spÃ©ciaux, conversion en minuscules, suppression des stopwords).
3. **Tokenisation** : Transformation des avis en tokens exploitables par BERT.
4. **InfÃ©rence** : PrÃ©diction du sentiment des avis en utilisant un modÃ¨le de classification binaire basÃ© sur BERT.
5. **Validation et tests** : Mise en place de tests unitaires pour chaque module du pipeline.

## ğŸ“‚ Structure du projet
```
analyse_des_sentiments/
â”‚â”€â”€ src/
â”‚   â”œâ”€â”€ data_extraction.py       # Extraction et validation des avis depuis dataset.csv
â”‚   â”œâ”€â”€ data_processing.py       # Nettoyage, suppression des stopwords et tokenisation des avis
â”‚   â”œâ”€â”€ inference.py             # InfÃ©rence et prÃ©diction du sentiment des avis
â”‚   â”œâ”€â”€ model.py                 # Chargement du modÃ¨le BERT pour la classification
â”‚
â”‚â”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”‚   â”œâ”€â”€ test_data_extraction.py  # Tests unitaires de data_extraction.py
â”‚   â”‚   â”œâ”€â”€ test_data_processing.py  # Tests unitaires de data_processing.py
â”‚   â”‚   â”œâ”€â”€ test_inference.py        # Tests unitaires de inference.py
â”‚   â”‚   â”œâ”€â”€ test_model.py            # Tests unitaires du modÃ¨le
â”‚
â”‚â”€â”€ dataset.csv                # DonnÃ©es brutes d'entraÃ®nement et de test
â”‚â”€â”€ dataset_tokenized.csv       # DonnÃ©es tokenisÃ©es et prÃ©traitÃ©es
â”‚â”€â”€ README.md                   # Documentation du projet
â”‚â”€â”€ requirements.txt            # DÃ©pendances Python requises
```

## âœ… AmÃ©liorations
- **Gestion des erreurs robuste** : Validation des donnÃ©es Ã  chaque Ã©tape du pipeline.
- **Optimisation des performances** : Utilisation de `torch.no_grad()` pour accÃ©lÃ©rer l'infÃ©rence.
- **Meilleure modularitÃ©** : Chaque Ã©tape du pipeline est encapsulÃ©e dans des modules distincts.
- **Tests unitaires complets** : Garantir la fiabilitÃ© du pipeline avec des tests automatisÃ©s.


  # Partie 2 :

  # ğŸ“Œ Analyse des Sentiments avec MLOps

## ğŸ“– Description du Projet
Ce projet met en place un pipeline MLOps pour l'analyse des sentiments, incluant le prÃ©traitement des donnÃ©es, l'entraÃ®nement du modÃ¨le, l'Ã©valuation et le monitoring automatique via GitHub Actions.

## ğŸ—ï¸ Architecture du Projet
Le projet est structurÃ© comme suit :
```
.
â”œâ”€â”€ .github/workflows/
â”‚   â”œâ”€â”€ build.yml         # Build et installation des dÃ©pendances
â”‚   â”œâ”€â”€ evaluate.yml      # Ã‰valuation automatique et monitoring des performances
â”‚   â”œâ”€â”€ release.yml       # DÃ©ploiement du modÃ¨le si la performance est satisfaisante
â”‚   â””â”€â”€ test.yml          # Tests unitaires et validation du pipeline
â”‚
â”œâ”€â”€ app.py                # API Flask pour faire des prÃ©dictions
â”œâ”€â”€ train.py              # Script d'entraÃ®nement du modÃ¨le
â”œâ”€â”€ evaluate.py           # Ã‰valuation du modÃ¨le sur les donnÃ©es test
â”œâ”€â”€ generate_evaluation_results.py  # GÃ©nÃ¨re les rÃ©sultats d'Ã©valuation en JSON
â”œâ”€â”€ generate_report.py     # Convertit les rÃ©sultats en CSV pour analyse
â”œâ”€â”€ monitor.py             # VÃ©rifie la performance du modÃ¨le
â”‚
â”œâ”€â”€ requirements.txt       # Liste des dÃ©pendances Python
â”œâ”€â”€ Dockerfile             # Conteneurisation de l'application
â”œâ”€â”€ tests/unit/            # Dossier contenant les tests unitaires
â”œâ”€â”€ README.md              # Documentation du projet
â””â”€â”€ .gitignore             # Fichiers Ã  exclure du dÃ©pÃ´t
```

## ğŸš€ Installation et ExÃ©cution
### 1ï¸âƒ£ PrÃ©requis
- Python 3.9+
- Docker (optionnel)
- GitHub Actions configurÃ©

### 2ï¸âƒ£ Installation des dÃ©pendances
```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ EntraÃ®nement du modÃ¨le
```bash
python train.py
```

### 4ï¸âƒ£ GÃ©nÃ©ration et Ã©valuation des rÃ©sultats
```bash
python generate_evaluation_results.py
python evaluate.py
python generate_report.py
```

### 5ï¸âƒ£ Lancer l'API Flask
```bash
python app.py
```

## ğŸ”¬ Monitoring et Performance
- **Ã‰valuation automatique** : `evaluate.yml` est exÃ©cutÃ© aprÃ¨s chaque entraÃ®nement.
- **Seuil de prÃ©cision** : Si la prÃ©cision est infÃ©rieure Ã  `0.75`, une alerte est gÃ©nÃ©rÃ©e.
- **Rapport CSV** : `performance_report.csv` est crÃ©Ã© pour analyser les rÃ©sultats.

## ğŸ“Š Automatisation avec GitHub Actions
Les workflows sont dÃ©finis dans `.github/workflows/`. Voici le pipeline automatisÃ© :
1. **build.yml** â†’ Installe les dÃ©pendances et valide le code
2. **test.yml** â†’ ExÃ©cute les tests unitaires
3. **evaluate.yml** â†’ Ã‰value le modÃ¨le et surveille ses performances
4. **release.yml** â†’ DÃ©ploie le modÃ¨le si les performances sont bonnes

## ğŸ“Œ ExÃ©cution sur GitHub Actions
Pour exÃ©cuter `evaluate.yml` automatiquement :
```yaml
on:
  workflow_run:
    workflows: ["Build"]
    types:
      - completed
```

## ğŸ’¡ AmÃ©liorations possibles
- Ajouter une alerte Slack pour les performances critiques
- Enregistrer les mÃ©triques dans une base de donnÃ©es
- AmÃ©liorer la robustesse du modÃ¨le avec des donnÃ©es augmentÃ©es

---
ğŸš€ **Projet dÃ©veloppÃ© avec une approche MLOps pour une gestion efficace du cycle de vie du modÃ¨le !**


